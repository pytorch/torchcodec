{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Streaming data through file-like support\n\nIn this example, we will show how to decode streaming data. That is, when files\ndo not reside locally, we will show how to only download the data segments that\nare needed to decode the frames you care about. We accomplish this capability\nwith Python\n[file-like objects](https://docs.python.org/3/glossary.html#term-file-like-object).\nOur example uses a video file, so we use the :class:`~torchcodec.decoders.VideoDecoder`\nclass to decode it. But all of the lessons here also apply to audio files and the\n:class:`~torchcodec.decoders.AudioDecoder` class as well.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, a bit of boilerplate. We define two functions: one to download content\nfrom a given URL, and another to time the execution of a given function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport requests\nfrom time import perf_counter_ns\n\n\ndef get_url_content(url):\n    response = requests.get(url, headers={\"User-Agent\": \"\"})\n    if response.status_code != 200:\n        raise RuntimeError(f\"Failed to download video. {response.status_code = }.\")\n    return response.content\n\n\ndef bench(f, average_over=10, warmup=2):\n    for _ in range(warmup):\n        f()\n\n    times = []\n    for _ in range(average_over):\n        start = perf_counter_ns()\n        f()\n        end = perf_counter_ns()\n        times.append(end - start)\n\n    times = torch.tensor(times) * 1e-6  # ns to ms\n    std = times.std().item()\n    med = times.median().item()\n    print(f\"{med = :.2f}ms +- {std:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance: downloading first vs. streaming\n\nWe are going to investigate the cost of having to download an entire video\nbefore decoding any frames versus being able to stream the video's data\nwhile decoding. To demonsrate an extreme case, we're going to always decode\njust the first frame of the video, while we vary how we get that video's\ndata.\n\nThe video we're going to use in this tutorial is publicly available on the\ninternet. We perform an initial download of it so that we can understand\nits size and content:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchcodec.decoders import VideoDecoder\n\nnasa_url = \"https://download.pytorch.org/torchaudio/tutorial-assets/stream-api/NASAs_Most_Scientifically_Complex_Space_Observatory_Requires_Precision-MP4.mp4\"\n\npre_downloaded_raw_video_bytes = get_url_content(nasa_url)\ndecoder = VideoDecoder(pre_downloaded_raw_video_bytes)\n\nprint(f\"Video size in MB: {len(pre_downloaded_raw_video_bytes) // 1024 // 1024}\")\nprint(decoder.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the video is about 253 MB, has the resolution 1920x1080, is\nabout 30 frames per second and is almost 3 and a half minutes long. As we\nonly want to decode the first frame, we would clearly benefit from not having\nto download the entire video!\n\nLet's first test three scenarios:\n\n  1. Decode from the *existing* video we just downloaded. This is our baseline\n     performance, as we've reduced the downloading cost to 0.\n  2. Download the entire video before decoding. This is the worst case\n     that we want to avoid.\n  3. Provde the URL directly to the :class:`~torchcodec.decoders.VideoDecoder` class, which will pass\n     the URL on to FFmpeg. Then FFmpeg will decide how much of the video to\n     download before decoding.\n\nNote that in our scenarios, we are always setting the ``seek_mode`` parameter of\nthe :class:`~torchcodec.decoders.VideoDecoder` class to ``\"approximate\"``. We do\nthis to avoid scanning the entire video during initialization, which would\nrequire downloading the entire video even if we only want to decode the first\nframe. See `sphx_glr_generated_examples_approximate_mode.py` for more.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def decode_from_existing_download():\n    decoder = VideoDecoder(\n        source=pre_downloaded_raw_video_bytes,\n        seek_mode=\"approximate\",\n    )\n    return decoder[0]\n\n\ndef download_before_decode():\n    raw_video_bytes = get_url_content(nasa_url)\n    decoder = VideoDecoder(\n        source=raw_video_bytes,\n        seek_mode=\"approximate\",\n    )\n    return decoder[0]\n\n\ndef direct_url_to_ffmpeg():\n    decoder = VideoDecoder(\n        source=nasa_url,\n        seek_mode=\"approximate\",\n    )\n    return decoder[0]\n\n\nprint(\"Decode from existing download:\")\nbench(decode_from_existing_download)\nprint()\n\nprint(\"Download before decode:\")\nbench(download_before_decode)\nprint()\n\nprint(\"Direct url to FFmpeg:\")\nbench(direct_url_to_ffmpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Decoding the already downloaded video is clearly the fastest. Having to\ndownload the entire video each time we want to decode just the first frame\nis many times slower than decoding an existing video. Providing a direct URL\nis much better, but we're still probably downloading more than we need to.\n\nWe can do better, and the way how is to use a file-like object which\nimplements its own read and seek methods that only download data from a URL as\nneeded. Rather than implementing our own, we can use such objects from the\n[fsspec](https://github.com/fsspec/filesystem_spec) module that provides\n[Filesystem interfaces for Python](https://filesystem-spec.readthedocs.io/en/latest/?badge=latest).\nNote that using these capabilities from the fsspec library also requires the\n[aiohttp](https://docs.aiohttp.org/en/stable/) module. You can install both with\n`pip install fsspec aiohttp`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import fsspec\n\n\ndef stream_while_decode():\n    # The `client_kwargs` are passed down to the aiohttp module's client\n    # session; we need to indicate that we need to trust the environment\n    # settings for proxy configuration. Depending on your environment, you may\n    # not need this setting.\n    with fsspec.open(nasa_url, client_kwargs={'trust_env': True}) as file_like:\n        decoder = VideoDecoder(file_like, seek_mode=\"approximate\")\n        return decoder[0]\n\n\nprint(\"Stream while decode: \")\nbench(stream_while_decode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Streaming the data through a file-like object is much faster than\ndownloading the video first. And not only is it also faster than\nproviding a direct URL, it's more general. :class:`~torchcodec.decoders.VideoDecoder` supports\ndirect URLs because the underlying FFmpeg functions support them. But the\nkinds of protocols supported are determined by what that version of FFmpeg\nsupports. A file-like object can adapt any kind of resource, including ones\nthat are specific to your own infrastructure and are unknown to FFmpeg.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How it works\nIn Python, a [file-like object](https://docs.python.org/3/glossary.html#term-file-like-object)\nis any object that exposes special methods for reading, writing and seeking.\nWhile such methods are obviously file oriented, it's not required that\na file-like object is backed by an actual file. As far as Python is concerned,\nif an object acts like a file, it's a file. This is a powerful concept, as\nit enables libraries that read or write data to assume a file-like interface.\nOther libraries that present novel resources can then be easily used by\nproviding a file-like wrapper for their resource.\n\nFor our case, we only need the read and seek methods for decoding. The exact\nmethod signature needed is in the example below. Rather than wrap a novel\nresource, we demonstrate this capability by wrapping an actual file while\ncounting how often each method is called.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\nimport tempfile\n\n# Create a local file to interact with.\ntemp_dir = tempfile.mkdtemp()\nnasa_video_path = Path(temp_dir) / \"nasa_video.mp4\"\nwith open(nasa_video_path, \"wb\") as f:\n    f.write(pre_downloaded_raw_video_bytes)\n\n\n# A file-like class that is backed by an actual file, but it intercepts reads\n# and seeks to maintain counts.\nclass FileOpCounter:\n    def __init__(self, file):\n        self._file = file\n        self.num_reads = 0\n        self.num_seeks = 0\n\n    def read(self, size: int) -> bytes:\n        self.num_reads += 1\n        return self._file.read(size)\n\n    def seek(self, offset: int, whence: int) -> bytes:\n        self.num_seeks += 1\n        return self._file.seek(offset, whence)\n\n\n# Let's now get a file-like object from our class defined above, providing it a\n# reference to the file we created. We pass our file-like object to the decoder\n# rather than the file itself.\nfile_op_counter = FileOpCounter(open(nasa_video_path, \"rb\"))\ncounter_decoder = VideoDecoder(file_op_counter, seek_mode=\"approximate\")\n\nprint(\"Decoder initialization required \"\n      f\"{file_op_counter.num_reads} reads and \"\n      f\"{file_op_counter.num_seeks} seeks.\")\n\ninit_reads = file_op_counter.num_reads\ninit_seeks = file_op_counter.num_seeks\n\nfirst_frame = counter_decoder[0]\n\nprint(\"Decoding the first frame required \"\n      f\"{file_op_counter.num_reads - init_reads} additional reads and \"\n      f\"{file_op_counter.num_seeks - init_seeks} additional seeks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While we defined a simple class primarily for demonstration, it's actually\nuseful for diagnosing how much reading and seeking are required for different\ndecoding operations. We've also introduced a mystery that we should answer:\nwhy does *initializing* the decoder take more reads and seeks than decoding\nthe first frame? The answer is that in our decoder implementation, we're\nactually calling a special\n[FFmpeg function](https://ffmpeg.org/doxygen/6.1/group__lavf__decoding.html#gad42172e27cddafb81096939783b157bb)\nthat decodes the first few frames to return more robust metadata.\n\nIt's also worth noting that the Python file-like interface is only half of\nthe story. FFmpeg also has its own mechanism for directing reads and seeks\nduring decoding to user-define functions. The\n:class:`~torchcodec.decoders.VideoDecoder` object does the work of\nconnecting the Python methods you define to FFmpeg. All you have to do is\ndefine your methods in Python, and we do the rest.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance: local file path vs. local file-like object\n\nSince we have a local file defined, let's do a bonus performance test. We now\nhave two means of providing a local file to :class:`~torchcodec.decoders.VideoDecoder`:\n\n  1. Through a *path*, where the :class:`~torchcodec.decoders.VideoDecoder`\n     object will then do the work of opening the local file at that path.\n  2. Through a *file-like object*, where you open the file yourself and provide\n     the file-like object to :class:`~torchcodec.decoders.VideoDecoder`.\n\nAn obvious question is: which is faster? The code below tests that question.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def decode_from_existing_file_path():\n    decoder = VideoDecoder(nasa_video_path, seek_mode=\"approximate\")\n    return decoder[0]\n\n\ndef decode_from_existing_open_file_object():\n    with open(nasa_video_path, \"rb\") as file:\n        decoder = VideoDecoder(file, seek_mode=\"approximate\")\n        return decoder[0]\n\n\nprint(\"Decode from existing file path:\")\nbench(decode_from_existing_file_path)\nprint()\n\nprint(\"Decode from existing open file object:\")\nbench(decode_from_existing_open_file_object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thankfully, the answer is both means of decoding from a local file take about\nthe same amount of time. This result means that in your own code, you can use\nwhichever method is more convienient. What this result implies is that the\ncost of actually reading and copying data dominates the cost of calling Python\nmethods while decoding.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's clean up the local resources we created.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import shutil\nshutil.rmtree(temp_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}