import importlib
import json
import os
import pathlib
import subprocess
import sys

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Union

import numpy as np
import pytest

import torch

from torchcodec._core import get_ffmpeg_library_versions
from torchcodec.decoders._video_decoder import _read_custom_frame_mappings

IS_WINDOWS = sys.platform in ("win32", "cygwin")


# Decorator for skipping CUDA tests when CUDA isn't available. The tests are
# effectively marked to be skipped in pytest_collection_modifyitems() of
# conftest.py
def needs_cuda(test_item):
    return pytest.mark.needs_cuda(test_item)


def all_supported_devices():
    return ("cpu", pytest.param("cuda", marks=pytest.mark.needs_cuda))


def get_ffmpeg_major_version():
    ffmpeg_version = get_ffmpeg_library_versions()["ffmpeg_version"]
    # When building FFmpeg from source there can be a `n` prefix in the version
    # string.  This is quite brittle as we're using av_version_info(), which has
    # no stable format. See https://github.com/pytorch/torchcodec/issues/100
    if ffmpeg_version.startswith("n"):
        ffmpeg_version = ffmpeg_version.removeprefix("n")
    return int(ffmpeg_version.split(".")[0])


def cuda_version_used_for_building_torch() -> Optional[tuple[int, int]]:
    # Return the CUDA version that was used to build PyTorch. That's not always
    # the same as the CUDA version that is currently installed on the running
    # machine, which is what we actually want. On the CI though, these are the
    # same.
    if torch.version.cuda is None:
        return None
    else:
        return tuple(int(x) for x in torch.version.cuda.split("."))


def psnr(a, b, max_val=255) -> float:
    # Return Peak Signal-to-Noise Ratio (PSNR) between two tensors a and b. The
    # higher, the better.
    # According to https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio,
    # typical values for the PSNR in lossy image and video compression are
    # between 30 and 50 dB.
    # Acceptable values for wireless transmission quality loss are considered to
    # be about 20 dB to 25 dB
    mse = torch.mean((a.float() - b.float()) ** 2)
    if mse == 0:
        return float("inf")
    return 20 * torch.log10(max_val / torch.sqrt(mse)).item()


# For use with decoded data frames. `psnr` sets the PSNR threshold when
# frames are considered equal. `float("inf")` correspond to bit-to-bit
# identical frames. Function returns calculated psnr value.
def assert_frames_equal(input, other, psnr=40, msg=None):
    if torch.allclose(input, other, atol=0, rtol=0):
        return float("inf")
    else:
        from torcheval.metrics import PeakSignalNoiseRatio

        metric = PeakSignalNoiseRatio()
        metric.update(input, other)
        m = metric.compute()
        message = f"low psnr: {m} < {psnr}"
        if (msg):
            message += f" ({msg})"
        assert m >= psnr, message
        return m


# Asserts that at least `percentage`% of the values are within the absolute tolerance.
# Percentage is expected in [0, 100] (actually, [60, 100])
def assert_tensor_close_on_at_least(
    actual_tensor, ref_tensor, *, percentage, atol, **kwargs
):
    # In theory lower bound should be 0, but we want to make sure we don't
    # mistakenly pass percentage in [0, 1]
    assert 60 < percentage <= 100, (
        f"Percentage must be in [60, 100], got {percentage}. "
        "Are you sure setting such a low tolerance is desired?"
    )
    assert (
        actual_tensor.device == ref_tensor.device
    ), f"Devices don't match: {actual_tensor.device} vs {ref_tensor.device}"

    abs_diff = (ref_tensor.float() - actual_tensor.float()).abs()
    valid_percentage = (abs_diff <= atol).float().mean() * 100
    if valid_percentage < percentage:
        raise AssertionError(
            f"Expected at least {percentage}% of values to be within atol={atol}, "
            f"but only {valid_percentage}% were."
        )


def in_fbcode() -> bool:
    return os.environ.get("IN_FBCODE_TORCHCODEC") == "1"


def _get_file_path(filename: str) -> pathlib.Path:
    if in_fbcode():
        resource = (
            importlib.resources.files(__spec__.parent)
            .joinpath("resources")
            .joinpath(filename)
        )
        with importlib.resources.as_file(resource) as path:
            return path
    else:
        return pathlib.Path(__file__).parent / "resources" / filename


@dataclass
class TestFrameInfo:
    pts_seconds: float
    duration_seconds: float


@dataclass
class TestVideoStreamInfo:
    width: int
    height: int
    num_color_channels: int


@dataclass
class TestAudioStreamInfo:
    sample_rate: int
    num_channels: int
    duration_seconds: float
    num_frames: int
    sample_format: str


@dataclass
class TestContainerFile:
    __test__ = False  # prevents pytest from thinking this is a test class

    filename: str

    default_stream_index: int
    stream_infos: Dict[int, Union[TestVideoStreamInfo, TestAudioStreamInfo]]
    frames: Dict[int, Dict[int, TestFrameInfo]]
    _custom_frame_mappings_data: Dict[
        int, Optional[tuple[torch.Tensor, torch.Tensor, torch.Tensor]]
    ] = field(default_factory=dict)

    def __post_init__(self):
        # We load the .frames attribute from the checked-in json files, if needed.
        # These frame info files are dumped with ffprobe, e.g.:
        # ```
        # ffprobe -v error -hide_banner -select_streams v:1 -show_frames -of json test/resources/nasa_13013.mp4 | jq '[.frames[] | {duration_time, pts_time}]'
        # ```
        # This will output the metadata for the frames of the second video
        # stream (v:1). First audio stream would be a:0.
        # Note that we are using the absolute stream index in the file. But
        # ffprobe uses a relative stream for that media type.
        for stream_index in self.stream_infos:
            if stream_index in self.frames:
                # .frames may be manually set: for some streams, we don't need
                # the info for all frames. We don't need to load anything in
                # this case
                continue

            frames_info_path = _get_file_path(
                f"{self.filename}.stream{stream_index}.all_frames_info.json"
            )

            if not frames_info_path.exists():
                raise ValueError(
                    f"Couldn't find {frames_info_path} for {self.filename}. "
                    "You need to submit this file, or specify the `frames` field manually."
                )

            with open(frames_info_path, "r") as f:
                frames_info = json.loads(f.read())
            self.frames[stream_index] = {
                frame_index: TestFrameInfo(
                    pts_seconds=float(frame_info["pts_time"]),
                    duration_seconds=float(frame_info["duration_time"]),
                )
                for frame_index, frame_info in enumerate(frames_info)
            }

    @property
    def path(self) -> pathlib.Path:
        return _get_file_path(self.filename)

    def to_tensor(self) -> torch.Tensor:
        arr = np.fromfile(self.path, dtype=np.uint8)
        return torch.from_numpy(arr)

    def get_frame_data_by_index(
        self, idx: int, *, stream_index: Optional[int] = None
    ) -> torch.Tensor:
        raise NotImplementedError("Override in child classes")

    def get_frame_data_by_range(
        self,
        start: int,
        stop: int,
        step: int = 1,
        *,
        stream_index: Optional[int] = None,
    ) -> torch.Tensor:
        raise NotImplementedError("Override in child classes")

    def get_pts_seconds_by_range(
        self,
        start: int,
        stop: int,
        step: int = 1,
        *,
        stream_index: Optional[int] = None,
    ) -> torch.Tensor:
        if stream_index is None:
            stream_index = self.default_stream_index

        all_pts = [
            self.frames[stream_index][i].pts_seconds for i in range(start, stop, step)
        ]
        return torch.tensor(all_pts, dtype=torch.float64)

    def get_duration_seconds_by_range(
        self,
        start: int,
        stop: int,
        step: int = 1,
        *,
        stream_index: Optional[int] = None,
    ) -> torch.Tensor:
        if stream_index is None:
            stream_index = self.default_stream_index

        all_durations = [
            self.frames[stream_index][i].duration_seconds
            for i in range(start, stop, step)
        ]
        return torch.tensor(all_durations, dtype=torch.float64)

    def get_frame_info(
        self, idx: int, *, stream_index: Optional[int] = None
    ) -> TestFrameInfo:
        if stream_index is None:
            stream_index = self.default_stream_index

        return self.frames[stream_index][idx]

    # This function is used to get the frame mappings for the custom_frame_mappings seek mode.
    def get_custom_frame_mappings(
        self, stream_index: Optional[int] = None
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        if stream_index is None:
            stream_index = self.default_stream_index
        if self._custom_frame_mappings_data.get(stream_index) is None:
            self._custom_frame_mappings_data[stream_index] = (
                _read_custom_frame_mappings(
                    self.generate_custom_frame_mappings(stream_index)
                )
            )
        return self._custom_frame_mappings_data[stream_index]

    def generate_custom_frame_mappings(self, stream_index: int) -> str:
        result = subprocess.run(
            [
                "ffprobe",
                "-i",
                f"{self.path}",
                "-select_streams",
                f"{stream_index}",
                "-show_frames",
                "-of",
                "json",
            ],
            check=True,
            capture_output=True,
            text=True,
        ).stdout
        return result

    @property
    def empty_pts_seconds(self) -> torch.Tensor:
        return torch.empty([0], dtype=torch.float64)

    @property
    def empty_duration_seconds(self) -> torch.Tensor:
        return torch.empty([0], dtype=torch.float64)


@dataclass
class TestVideo(TestContainerFile):
    """Base class for the *video* streams of a video container"""

    def get_frame_data_by_index(
        self, idx: int, *, stream_index: Optional[int] = None
    ) -> torch.Tensor:
        if stream_index is None:
            stream_index = self.default_stream_index

        file_path = _get_file_path(
            f"{self.filename}.stream{stream_index}.frame{idx:06d}.pt"
        )
        return torch.load(file_path, weights_only=True).permute(2, 0, 1)

    def get_frame_data_by_range(
        self,
        start: int,
        stop: int,
        step: int = 1,
        *,
        stream_index: Optional[int] = None,
    ) -> torch.Tensor:
        tensors = [
            self.get_frame_data_by_index(i, stream_index=stream_index)
            for i in range(start, stop, step)
        ]
        return torch.stack(tensors)

    @property
    def width(self) -> int:
        return self.stream_infos[self.default_stream_index].width

    @property
    def height(self) -> int:
        return self.stream_infos[self.default_stream_index].height

    @property
    def num_color_channels(self) -> int:
        return self.stream_infos[self.default_stream_index].num_color_channels

    @property
    def empty_chw_tensor(self) -> torch.Tensor:
        return torch.empty(
            [0, self.num_color_channels, self.height, self.width], dtype=torch.uint8
        )

    def get_width(self, *, stream_index: Optional[int]) -> int:
        if stream_index is None:
            stream_index = self.default_stream_index

        return self.stream_infos[stream_index].width

    def get_height(self, *, stream_index: Optional[int] = None) -> int:
        if stream_index is None:
            stream_index = self.default_stream_index

        return self.stream_infos[stream_index].height

    def get_num_color_channels(self, *, stream_index: Optional[int] = None) -> int:
        if stream_index is None:
            stream_index = self.default_stream_index

        return self.stream_infos[stream_index].num_color_channels

    def get_empty_chw_tensor(self, *, stream_index: int) -> torch.Tensor:
        return torch.empty(
            [
                0,
                self.get_num_color_channels(stream_index=stream_index),
                self.get_height(stream_index=stream_index),
                self.get_width(stream_index=stream_index),
            ],
            dtype=torch.uint8,
        )


NASA_VIDEO = TestVideo(
    filename="nasa_13013.mp4",
    default_stream_index=3,
    stream_infos={
        0: TestVideoStreamInfo(width=320, height=180, num_color_channels=3),
        3: TestVideoStreamInfo(width=480, height=270, num_color_channels=3),
    },
    frames={},  # Automatically loaded from json file
)

# Video generated with:
# ffmpeg -f lavfi -i testsrc2=duration=1:size=200x200:rate=30 -c:v libx265 -pix_fmt yuv420p10le -preset fast -crf 23 h265_10bits.mp4
H265_10BITS = TestVideo(
    filename="h265_10bits.mp4",
    default_stream_index=0,
    stream_infos={
        0: TestVideoStreamInfo(width=200, height=200, num_color_channels=3),
    },
    frames={0: {}},  # Not needed yet
)

# Video generated with:
# peg -f lavfi -i testsrc2=duration=1:size=200x200:rate=30 -c:v libx264 -pix_fmt yuv420p10le -preset fast -crf 23 h264_10bits.mp4
H264_10BITS = TestVideo(
    filename="h264_10bits.mp4",
    default_stream_index=0,
    stream_infos={
        0: TestVideoStreamInfo(width=200, height=200, num_color_channels=3),
    },
    frames={0: {}},  # Not needed yet
)


@dataclass
class TestAudio(TestContainerFile):
    """Base class for the *audio* streams of a container (potentially a video),
    or a pure audio file"""

    stream_infos: Dict[int, TestAudioStreamInfo]
    # stream_index -> list of 2D frame tensors of shape (num_channels, num_samples_in_that_frame)
    # num_samples_in_that_frame isn't necessarily constant for a given stream.
    _reference_frames: Dict[int, List[torch.Tensor]] = field(default_factory=dict)

    # Storing each individual frame is too expensive for audio, because there's
    # a massive overhead in the binary format saved by pytorch. Saving all the
    # frames in a single file uses 1.6MB while saving all frames in individual
    # files uses 302MB (yes).
    # So we store the reference frames in a single file, and load/cache those
    # when the TestAudio instance is created.
    def __post_init__(self):
        super().__post_init__()
        for stream_index in self.stream_infos:
            frames_data_path = _get_file_path(
                f"{self.filename}.stream{stream_index}.all_frames.pt"
            )

            if frames_data_path.exists():
                # To ease development, we allow for the reference frames not to
                # exist. It means the asset cannot be used to check validity of
                # decoded frames.
                self._reference_frames[stream_index] = torch.load(
                    frames_data_path, weights_only=True
                )

    def get_frame_data_by_index(
        self, idx: int, *, stream_index: Optional[int] = None
    ) -> torch.Tensor:
        if stream_index is None:
            stream_index = self.default_stream_index

        return self._reference_frames[stream_index][idx]

    def get_frame_data_by_range(
        self,
        start: int,
        stop: int,
        step: int = 1,
        *,
        stream_index: Optional[int] = None,
    ) -> torch.Tensor:
        tensors = [
            self.get_frame_data_by_index(i, stream_index=stream_index)
            for i in range(start, stop, step)
        ]
        return torch.cat(tensors, dim=-1)

    def get_frame_index(
        self, *, pts_seconds: float, stream_index: Optional[int] = None
    ) -> int:
        if stream_index is None:
            stream_index = self.default_stream_index

        if pts_seconds <= self.frames[stream_index][0].pts_seconds:
            # Special case for e.g. NASA_AUDIO_MP3 whose first frame's pts is
            # 0.13~, not 0.
            return 0
        try:
            # Could use bisect() to maek this faster if needed
            return next(
                frame_index
                for (frame_index, frame_info) in self.frames[stream_index].items()
                if frame_info.pts_seconds
                <= pts_seconds
                < frame_info.pts_seconds + frame_info.duration_seconds
            )
        except StopIteration:
            return len(self.frames[stream_index]) - 1

    @property
    def sample_rate(self) -> int:
        return self.stream_infos[self.default_stream_index].sample_rate

    @property
    def num_channels(self) -> int:
        return self.stream_infos[self.default_stream_index].num_channels

    @property
    def duration_seconds(self) -> float:
        return self.stream_infos[self.default_stream_index].duration_seconds

    @property
    def num_frames(self) -> int:
        return self.stream_infos[self.default_stream_index].num_frames

    @property
    def sample_format(self) -> str:
        return self.stream_infos[self.default_stream_index].sample_format


# This file was generated with:
# ffmpeg -y -i test/resources/nasa_13013.mp4 -b:a 192K -vn test/resources/nasa_13013.mp4.audio.mp3"
NASA_AUDIO_MP3 = TestAudio(
    filename="nasa_13013.mp4.audio.mp3",
    default_stream_index=0,
    frames={},  # Automatically loaded from json file
    stream_infos={
        0: TestAudioStreamInfo(
            sample_rate=8_000,
            num_channels=2,
            duration_seconds=13.248,
            num_frames=183,
            sample_format="fltp",
        )
    },
)

# This file is the same as NASA_AUDIO_MP3, with a sample rate of 44_100. It was generated with:
# ffmpeg -i test/resources/nasa_13013.mp4.audio.mp3 -ar 44100 test/resources/nasa_13013.mp4.audio_44100.mp3
NASA_AUDIO_MP3_44100 = TestAudio(
    filename="nasa_13013.mp4.audio_44100.mp3",
    default_stream_index=0,
    frames={},  # Automatically loaded from json file
    stream_infos={
        0: TestAudioStreamInfo(
            sample_rate=44_100,
            num_channels=2,
            duration_seconds=13.09,
            num_frames=501,
            sample_format="fltp",
        )
    },
)

NASA_AUDIO = TestAudio(
    filename="nasa_13013.mp4",
    default_stream_index=4,
    frames={},  # Automatically loaded from json file
    stream_infos={
        4: TestAudioStreamInfo(
            sample_rate=16_000,
            num_channels=2,
            duration_seconds=13.056,
            num_frames=204,
            sample_format="fltp",
        )
    },
)

# Note that the file itself is s32 sample format, but the reference frames are
# stored as fltp. We can add the s32 original reference frames once we support
# decoding to non-fltp format, but for now we don't need to.
SINE_MONO_S32 = TestAudio(
    filename="sine_mono_s32.wav",
    default_stream_index=0,
    frames={},  # Automatically loaded from json file
    stream_infos={
        0: TestAudioStreamInfo(
            sample_rate=16_000,
            num_channels=1,
            duration_seconds=4,
            num_frames=63,
            sample_format="s32",
        )
    },
)

# This file is an upsampled version of SINE_MONO_S32, generated with:
# ffmpeg -i test/resources/sine_mono_s32.wav -ar 44100 -c:a pcm_s32le test/resources/sine_mono_s32_44100.wav
SINE_MONO_S32_44100 = TestAudio(
    filename="sine_mono_s32_44100.wav",
    default_stream_index=0,
    frames={},  # Automatically loaded from json file
    stream_infos={
        0: TestAudioStreamInfo(
            sample_rate=44_100,
            num_channels=1,
            duration_seconds=4,
            num_frames=173,
            sample_format="s32",
        )
    },
)

# This file is a downsampled version of SINE_MONO_S32, generated with:
# ffmpeg -i test/resources/sine_mono_s32.wav -ar 8000 -c:a pcm_s32le test/resources/sine_mono_s32_8000.wav
SINE_MONO_S32_8000 = TestAudio(
    filename="sine_mono_s32_8000.wav",
    default_stream_index=0,
    frames={},  # Automatically loaded from json file
    stream_infos={
        0: TestAudioStreamInfo(
            sample_rate=8000,
            num_channels=1,
            duration_seconds=4,
            num_frames=32,
            sample_format="s32",
        )
    },
)

# Same sample rate as SINE_MONO_S32, but encoded as s16 instead of s32. Generated with:
# ffmpeg -i test/resources/sine_mono_s32.wav -ar 16000 -c:a pcm_s16le test/resources/sine_mono_s16.wav
SINE_MONO_S16 = TestAudio(
    filename="sine_mono_s16.wav",
    default_stream_index=0,
    frames={},  # Automatically loaded from json file
    stream_infos={
        0: TestAudioStreamInfo(
            sample_rate=16_000,
            num_channels=1,
            duration_seconds=4,
            num_frames=63,
            sample_format="s16",
        )
    },
)

H265_VIDEO = TestVideo(
    filename="h265_video.mp4",
    default_stream_index=0,
    # This metadata is extracted manually.
    #  $ ffprobe -v error -hide_banner -select_streams v:0 -show_frames -of json test/resources/h265_video.mp4 > out.json
    stream_infos={
        0: TestVideoStreamInfo(width=128, height=128, num_color_channels=3),
    },
    frames={
        0: {
            6: TestFrameInfo(pts_seconds=0.6, duration_seconds=0.1),
        },
    },
)

AV1_VIDEO = TestVideo(
    filename="av1_video.mkv",
    default_stream_index=0,
    # This metadata is extracted manually.
    #  $ ffprobe -v error -hide_banner -select_streams v:0 -show_frames -of json test/resources/av1_video.mkv > out.json
    stream_infos={
        0: TestVideoStreamInfo(width=640, height=360, num_color_channels=3),
    },
    frames={
        0: {
            10: TestFrameInfo(pts_seconds=0.400000, duration_seconds=0.040000),
        },
    },
)


# This is a BT.709 full range video, generated with:
# ffmpeg -f lavfi -i testsrc2=duration=1:size=1920x720:rate=30 \
# -c:v libx264 -pix_fmt yuv420p -color_primaries bt709 -color_trc bt709 \
# -colorspace bt709 -color_range pc bt709_full_range.mp4
#
# We can confirm the color space and color range with:
# ffprobe -v quiet -select_streams v:0 -show_entries stream=color_space,color_transfer,color_primaries,color_range -of default=noprint_wrappers=1 test/resources/bt709_full_range.mp4
# color_range=pc
# color_space=bt709
# color_transfer=bt709
# color_primaries=bt709
BT709_FULL_RANGE = TestVideo(
    filename="bt709_full_range.mp4",
    default_stream_index=0,
    stream_infos={
        0: TestVideoStreamInfo(width=1280, height=720, num_color_channels=3),
    },
    frames={0: {}},  # Not needed for now
)
