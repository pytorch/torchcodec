{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Decoding a video with VideoDecoder\n\nIn this example, we'll learn how to decode a video using the\n:class:`~torchcodec.decoders.VideoDecoder` class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, a bit of boilerplate: we'll download a video from the web, and define a\nplotting utility. You can ignore that part and jump right below to\n`creating_decoder`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Optional\nimport torch\nimport requests\n\n\n# Video source: https://www.pexels.com/video/dog-eating-854132/\n# License: CC0. Author: Coverr.\nurl = \"https://videos.pexels.com/video-files/854132/854132-sd_640_360_25fps.mp4\"\nresponse = requests.get(url, headers={\"User-Agent\": \"\"})\nif response.status_code != 200:\n    raise RuntimeError(f\"Failed to download video. {response.status_code = }.\")\n\nraw_video_bytes = response.content\n\n\ndef plot(frames: torch.Tensor, title : Optional[str] = None):\n    try:\n        from torchvision.utils import make_grid\n        from torchvision.transforms.v2.functional import to_pil_image\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print(\"Cannot plot, please run `pip install torchvision matplotlib`\")\n        return\n\n    plt.rcParams[\"savefig.bbox\"] = 'tight'\n    fig, ax = plt.subplots()\n    ax.imshow(to_pil_image(make_grid(frames)))\n    ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n    if title is not None:\n        ax.set_title(title)\n    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Creating a decoder\n\nWe can now create a decoder from the raw (encoded) video bytes. You can of\ncourse use a local video file and pass the path as input, rather than download\na video.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torchcodec.decoders import VideoDecoder\n\n# You can also pass a path to a local file!\ndecoder = VideoDecoder(raw_video_bytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The has not yet been decoded by the decoder, but we already have access to\nsome metadata via the ``metadata`` attribute which is a\n:class:`~torchcodec.decoders.VideoStreamMetadata` object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(decoder.metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoding frames by indexing the decoder\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "first_frame = decoder[0]  # using a single int index\nevery_twenty_frame = decoder[0 : -1 : 20]  # using slices\n\nprint(f\"{first_frame.shape = }\")\nprint(f\"{first_frame.dtype = }\")\nprint(f\"{every_twenty_frame.shape = }\")\nprint(f\"{every_twenty_frame.dtype = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Indexing the decoder returns the frames as :class:`torch.Tensor` objects.\nBy default, the shape of the frames is ``(N, C, H, W)`` where N is the batch\nsize C the number of channels, H is the height, and W is the width of the\nframes.  The batch dimension N is only present when we're decoding more than\none frame. The dimension order can be changed to ``N, H, W, C`` using the\n``dimension_order`` parameter of\n:class:`~torchcodec.decoders.VideoDecoder`. Frames are always of\n``torch.uint8`` dtype.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot(first_frame, \"First frame\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot(every_twenty_frame, \"Every 20 frame\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterating over frames\n\nThe decoder is a normal iterable object and can be iterated over like so:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for frame in decoder:\n    assert (\n        isinstance(frame, torch.Tensor)\n        and frame.shape == (3, decoder.metadata.height, decoder.metadata.width)\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieving pts and duration of frames\n\nIndexing the decoder returns pure :class:`torch.Tensor` objects. Sometimes, it\ncan be useful to retrieve additional information about the frames, such as\ntheir :term:`pts` (Presentation Time Stamp), and their duration.\nThis can be achieved using the\n:meth:`~torchcodec.decoders.VideoDecoder.get_frame_at` and\n:meth:`~torchcodec.decoders.VideoDecoder.get_frames_at`  methods, which will\nreturn a :class:`~torchcodec.Frame` and :class:`~torchcodec.FrameBatch`\nobjects respectively.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "last_frame = decoder.get_frame_at(len(decoder) - 1)\nprint(f\"{type(last_frame) = }\")\nprint(last_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "other_frames = decoder.get_frames_at([10, 0, 50])\nprint(f\"{type(other_frames) = }\")\nprint(other_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot(last_frame.data, \"Last frame\")\nplot(other_frames.data, \"Other frames\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Both :class:`~torchcodec.Frame` and\n:class:`~torchcodec.FrameBatch` have a ``data`` field, which contains\nthe decoded tensor data. They also have the ``pts_seconds`` and\n``duration_seconds`` fields which are single ints for\n:class:`~torchcodec.Frame`, and 1-D :class:`torch.Tensor` for\n:class:`~torchcodec.FrameBatch` (one value per frame in the batch).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using time-based indexing\n\nSo far, we have retrieved frames based on their index. We can also retrieve\nframes based on *when* they are played with\n:meth:`~torchcodec.decoders.VideoDecoder.get_frame_played_at` and\n:meth:`~torchcodec.decoders.VideoDecoder.get_frames_played_at`, which\nalso returns :class:`~torchcodec.Frame` and :class:`~torchcodec.FrameBatch`\nrespectively.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "frame_at_2_seconds = decoder.get_frame_played_at(seconds=2)\nprint(f\"{type(frame_at_2_seconds) = }\")\nprint(frame_at_2_seconds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "other_frames = decoder.get_frames_played_at(seconds=[10.1, 0.3, 5])\nprint(f\"{type(other_frames) = }\")\nprint(other_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot(frame_at_2_seconds.data, \"Frame played at 2 seconds\")\nplot(other_frames.data, \"Other frames\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}